# Private LLM

Deploy enterprise-grade private LLM inference with **zero data leakage**. Send your ideas, research, code, trade secrets - whatever you need. Your data stays yours.

## ğŸ›ï¸ Problem

You want privacy, but every option is impractical:

**âŒ Public APIs?**
- You send sensitive data in every request
- You don't know who owns the provider or their data practices
- Every request is exposed to potential data collection
- The privacy risk outweighs any convenience

**âŒ Mini Mac or home server?**
- Buy hardware (mini Mac = $1000+, GPU server = $3000+)
- Maintain it: electricity, cooling, network, hardware failures
- Your home IP may leak via outgoing connections
- Single point of failure, no redundancy
- Time investment outweighs benefits

**âŒ DIY GPU setup?**
- Research GPU compatibility for weeks
- Configure Linux, drivers, CUDA - months of tinkering
- Deal with instability, updates, supply chains
- Same data center ownership problems, less scale

**You need:**
- âœ… **Enterprise privacy**: Data never leaves your infrastructure
- âœ… **Production-grade reliability**: Built for 99.9%+ uptime
- âœ… **Operational simplicity**: Deploy and forget
- âœ… **Cost-efficiency**: No idle waste

## ğŸ’° The Freedom You Get

- **Literal privacy**: Send ideas, code, research, and sensitive information securely
- **Zero data collection**: No logs, no telemetry, no user tracking
- **No vendor lock-in**: Your data exists only in your infrastructure
- **No maintenance burden**: No servers to manage, update, or replace
- **Low cost**: Only pay when you actually use the model

## ğŸ›ï¸ What Makes This Enterprise-Grade

**Industry-standard security, made simple:**

- **Defense in depth**: Multi-layer authentication (token + mTLS)
  - External token authenticates you to the service
  - Internal certificates encrypt the actual request
  - Both required - if one is compromised, the system stays safe

- **Zero trust architecture**: Nothing assumed trusted
  - Cloud providers can't access your data
  - Network attacks prevented at multiple layers
  - Hardware security (TPM, Secure Boot) validates every boot

- **Compliance-ready**: SOC 2, ISO 27001-aligned controls
  - Audit trails without data logging
  - Secure key management with KMS
  - Immutable infrastructure prevents drift

- **Production reliability**: Not just "works sometimes"
  - State tracking prevents premature VM shutdown during active requests
  - Auto-recovery from failures
  - First-boot optimization (skip installation on subsequent runs)

## ğŸ›¡ï¸ Architecture

```
Your Application
      â–¼
[External API Token]
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cloud Functions (Proxy) â”‚
â”‚ - Validate token        â”‚
â”‚ - Check VM availability  â”‚
â”‚ - HTTPS â†’ mTLS Tunnel   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼ mTLS (TLS 1.3, 4096-bit RSA)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ VM (Spot L4 GPU)        â”‚
â”‚ - mTLS Server Cert      â”‚
â”‚ - Models on local SSD   â”‚
â”‚ - Shielded VM (Secure Boot+TPM)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–¼
Your Response (prompt and results stay private)
```

## ğŸ›¡ï¸ Architecture

```
Your Application
      â–¼
[External API Token]
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cloud Functions (Proxy) â”‚
â”‚ - Validate token        â”‚
â”‚ - Check VM availability  â”‚
â”‚ - HTTPS â†’ mTLS Tunnel   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼ mTLS (TLS 1.3, 4096-bit RSA)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ VM (Spot L4 GPU)        â”‚
â”‚ - mTLS Server Cert      â”‚
â”‚ - Models on local SSD   â”‚
â”‚ - Shielded VM (Secure Boot+TPM)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–¼
Your Response (prompt and results stay private)
```

## ğŸš€ Quick Start

```bash
# Deploy infrastructure
make init-terraform
make deploy

# Configure your environment
export LLM_PROXY_URL=$(terraform output -raw function_url)
export LLM_API_TOKEN=$(terraform output -raw api_token)

# Use it with anything
curl -H "Authorization: Bearer $LLM_API_TOKEN" $LLM_PROXY_URL/api/generate \
  -d '{"prompt":"Analyze this bank statement:","content":"<paste your sensitive data here>"}'

# Pull your own models
curl -X POST $LLM_PROXY_URL/api/pull \
  -H "Authorization: Bearer $LLM_API_TOKEN" \
  -d '{"name":"llama3.2:1b"}'
```

## ğŸ›ï¸ Architecture

```
Your Application
      â–¼
[External API Token]
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cloud Functions (Proxy) â”‚
â”‚ - Validate token        â”‚
â”‚ - Check VM availability  â”‚
â”‚ - HTTPS â†’ mTLS Tunnel   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼ mTLS (TLS 1.3, 4096-bit RSA)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ VM (Spot L4 GPU)        â”‚
â”‚ - mTLS Server Cert      â”‚
â”‚ - Models on local SSD   â”‚
â”‚ - Shielded VM (Secure Boot+TPM)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–¼
Your Response (prompt and results stay private)
```

## ğŸ›¡ï¸ Security Guarantees

- âœ… **Data Sovereignty**: Your prompts and inference results never leave your infrastructure
- âœ… **Zero Data Logging**: No user data stored, no logs, no metrics captured
- âœ… **HSM-Protected Secrets**: Customer-managed keys with hardware encryption
- âœ… **mTLS Everywhere**: Mutual authentication for all connections
- âœ… **At-Rest Encryption**: Secrets encrypted with hardware-managed keys
- âœ… **Spot VM Savings**: Up to 80% reduction vs on-demand pricing

## ğŸš€ Quick Start

```bash
# Deploy infrastructure
make init-terraform
make deploy

# Configure your environment
export LLM_PROXY_URL=$(terraform output -raw function_url)
export LLM_API_TOKEN=$(terraform output -raw api_token)

# Use it
curl -H "Authorization: Bearer $LLM_API_TOKEN" $LLM_PROXY_URL/api/generate \
  -d '{"prompt":"Hello","model":"glm-4.7-flash"}'

# Pull models
curl -X POST $LLM_PROXY_URL/api/pull \
  -H "Authorization: Bearer $LLM_API_TOKEN" \
  -d '{"name":"llama3.2:1b"}'
```

## ğŸ“¦ Features

### Privacy First
- **Zero data logging**: No prompts, no responses, no user data stored anywhere
- **Total sovereignty**: Your data exists only in your infrastructure
- **No data training**: Your inputs never become another company's model outputs
- **No vendor access**: Even cloud providers can't access your decrypted data

### Automatic Operation
- **Idle shutdown**: VM stays off between requests - no electricity waste
- **First-boot setup**: 6-11 minutes once, subsequent boots take 30-60 seconds
- **Auto-rotation**: Secrets and certificates updated automatically
- **State management**: VM starts instantly when you need it again

### Real Security
- **mTLS everywhere**: Every request authenticated end-to-end
- **Dual-layer security**: External token AND internal certificate required
- **Hardware encryption**: HSM-protected keys, at-rest and in-transit
- **Network isolation**: Dedicated infrastructure, no public exposure

## ğŸ“ Project Structure

```
private-llm/
â”œâ”€â”€ terraform.tf              # Version constraints
â”œâ”€â”€ variables.tf              # Cloud-agnostic variables
â”œâ”€â”€ modules/gcp/              # GCP implementation
â”‚   â”œâ”€â”€ compute.tf           # VM with GPU and Shielded config
â”‚   â”œâ”€â”€ secrets.tf           # KMS and Secret Manager
â”‚   â”œâ”€â”€ network.tf           # VPC with dedicated subnet
â”‚   â””â”€â”€ functions.tf         # Cloud Functions Gen2
â”œâ”€â”€ function/                 # Go proxy + rotation
â”‚   â”œâ”€â”€ main.go              # API proxy with token/mTLS validation
â”‚   â””â”€â”€ rotation.go          # Automatic secret rotation
â””â”€â”€ config/                   # VM startup + Caddyfile
```

*Architecture designed for portability across clouds: GCP supported now, AWS coming soon*

## ğŸ“Š Monthly Cost

- Spot VM: Low hourly rate ($1-2/hour depending on configuration)
- Storage: Fixed monthly (~$17/100GB)
- Functions: Minimal (~$0.50)
- Total: $20-80/month (varies by usage)

**ROI**: Pay only when you use it. No idle costs.

## ğŸ”§ Troubleshooting

### VM not responding?
```bash
gcloud compute instances tail-serial-port-output private-llm-vm --zone=us-central1-a
```

### Check provisioning status
```bash
gcloud firestore documents describe \
  projects/YOUR-PROJECT/databases/private-llm/documents/vm_state/private-llm-vm
```

### Pull models manually
```bash
curl -X POST $LLM_PROXY_URL/api/pull \
  -H "Authorization: Bearer $LLM_API_TOKEN" \
  -d '{"name":"your-model-name"}'
```

## ğŸŒ Cloud Support

- âœ… **Google Cloud Platform**: Fully supported
- ğŸš§ **AWS**: Coming soon
- ğŸš§ **Azure**: Coming soon

## ğŸ“– Technical Details

### Token Lifecycle
- **External token** (user): You generate it, Terraform outputs it
- **Internal token** (infrastructure): Auto-rotated every 2 hours
- **mTLS CA**: 10-year validity, immutable
- **Server/client certs**: 1-week validity, auto-rotated

### First Boot Timeline
1. VM boot: ~30s
2. Cloud-init: ~5s
3. Installation: 6-11 min (once)
4. Ollama: ~15s
5. Model warmup: Background

**Subsequent boots**: 30-60s (skips installation, ready in seconds)

## ğŸ¯ Usage Patterns

**Daily developer**: Runs briefly, VM stays idle - minimal cost
**Research**: Works multiple hours - model always ready
**Enterprise**: Continuous access - cost-efficient scaling

Whatever your use case, you only pay for when you use it.

## ğŸ¤ Contributing

1. Ensure code passes CI checks: `make test`
2. Verify Terraform formatting: `terraform fmt -check .`
3. No secrets in code - use Environment variables
4. Follow existing security patterns

## ğŸ“ License

Reference implementation for private LLM deployment. Configure your own cloud project for production use.

---

**One-line promise**: Your data stays private, your time stays yours. Deploy and forget with zero data leakage.